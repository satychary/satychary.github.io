<!-- ********************************************************************** -->
<!-- ********************************************************************** -->
<!-- ********************************************************************** -->
<!-- ********************************************************************** -->
<!-- ********************************************************************** -->
<!DOCTYPE html >
<html>
	<head>
		<meta name="generator" content=
		"HTML Tidy for Linux/x86 (vers 1st November 2003), see www.w3.org" />

		<!-- ------PAGE TITLE------- -->
		<title>
			Machine learning basics
		</title>
                
		<!-- ----------------------- -->

		<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
		<meta name="copyright" content=" "/>

		<!-- CSS, JS -->
		<link rel="stylesheet"  href="../../res/fonts_only.css"/>
<link rel="stylesheet"  href="../../res/layout_only.css"/>
<link rel="stylesheet" media="print" href="../../res/styles/slides/myslidy/bts/print.css"/>
<link rel="stylesheet" href="../../res/styles/hljs/default.min.css">
<script src="../../res/styles/slides/myslidy/bts/rollup.js" charset="utf-8" type="text/javascript"></script>
<script src="../../res/styles/slides/myslidy/bts/jsxgraphcore.js"></script>
<script src="../../res/styles/slides/myslidy/bts/SSS.js"></script>
<script src="../../res/styles/slides/myslidy/bts/strokeText.js"></script>
<script src="../../res/styles/slides/myslidy/bts/ASCIIMathML.js"></script>
<!-- http://www.wjagray.co.uk/maths/ASCIIMathTutorial.html -->
<script src="../../res/styles/js/MathJax/MathJax.js?config=AM_HTMLorMML-full"></script>  
<script src="../../res/styles/slides/myslidy/bts/DigitalClock.js"></script>
<script src="../../res/styles/hljs/highlight.min.js"></script>
	</head>

	<body>
		<!-- time, slide#, full-screen toggle, nav L.R -->
		<canvas style="opacity:0.85;z-index:10;width:400px;height:100px;position:absolute;margin-left:-100px;margin-top:-14px;" id='clockHolder' >
		</canvas>
		<script>
			setInterval("animateClock()", 1000);
		</script>
		<span id="slideNum" style="opacity:0.85;border:0px solid black; z-index:9;width:100px;height:10px;position:absolute;margin-left:12px;margin-top:16px;" >
		</span>
		<span style="opacity:0.25;z-index:100;border:0px solid black; width:50px;height:25px;position:absolute;margin-left:153px;margin-top:15px;" onclick="javascript:toggleView();" >
			***
		</span>
		<span style="font-size:30px;opacity:0.25;z-index:101;border:0px solid black; width:50px;height:25px;position:absolute;margin-left:12px;margin-top:60px;" onclick="javascript:previousSlide(true);" >
			&larr;
		</span>
		<span style="font-size:30px;opacity:0.25;z-index:100;border:0px solid black; width:50px;height:25px;position:absolute;margin-left:150px;margin-top:60px;" onclick="javascript:nextSlide(true);" >
			&rarr;
		</span>



		<!-- ************************** -->
		<!-- BEGIN SLIDES SLIDES SLIDES -->
		<!-- ************************** -->

		<!-- ****************************************** -->
		<!-- ****************************************** -->
		<!-- TITLE SLIDE -->
		<div class="slide">
			<table width="100%" height="100%" border="0" cellpadding="0" cellspacing="0">
				<tr>
					<td align="center" valign="center">
						<!-- valign was 'center' -->
						<!-- ---------- -->
						<br><br><br><br><br>
						<h1  class="title">
						<!-- ****************************** -->
							Machine learning
						</h1>

						<h1  class="subtitleC">
						<!-- ****************************** -->
                                                      A gentle introduction
						</h1>
                                                    <img src="pics/IsItAI.jpg" style="width: 50%">
                                                <h1 class="subtitleA">ML: BFD!! </h1>
						<!-- ****************************** -->
						<!-- ---------- -->
					</td>
				</tr>
			</table>
		</div><!-- osframe, slide -->
		<!-- END TITLE SLIDE -->
		<!-- ****************************************** -->
		<!-- ****************************************** -->



<!--
 - 
 - 
 - https://naokishibuya.medium.com/long-short-term-memory-394aa8461a35 [585]
 - [585]
 - [585] and https://www.tylervigen.com/spurious-correlations
 - https://blog.robertelder.org/how-to-make-a-cpu/ [CS100]
- https://spectrum.ieee.org/special-reports/the-great-ai-reckoning [585, 250, 100]
- https://realitone.com/products/realibanjo [100]
- https://www.weforum.org/agenda/2021/10/technology-trends-2021-mckinsey [585, 250, 100]



https://towardsdatascience.com/recent-developments-and-views-on-computer-vision-x-transformer-ed32a2c72654

https://theaisummer.com/transformers-computer-vision/?fbclid=IwAR2p_W2Tb7FtdIWV4QN9ok65kf-aooQ61P2acS9uj6NS0P1XWj4xQOiomtM

https://tamoghnasaha-22.medium.com/transformers-illustrated-5c9205a6c70f

https://www.neuralearn.ai/course_page/7/?fbclid=IwAR3Pueo8-42V7kRvisaOPWbEQ_Jqh2GPiN6xbr_NMUQXd3o0jE3ScGq5Ows

https://www.youtube.com/watch?v=5ocQmDsJfYU

https://www.routledge.com/go/explorations-in-ai-and-machine-learning-freebook?fbclid=IwAR33cYfjQh_wZh_lmY41N2lG99xflOAYQ-U28ZWQenjketnFA6hlkEKD0JI

https://medium.com/@mohamedhedifkir/generative-adversarial-nets-paper-6d3db6aef417

https://arxiv.org/abs/2103.01209?fbclid=IwAR2sJSWC50o-Sr26OvcGYF1OLl-EfxM20npqEnkbPgnEonwux_vYqNjblOQ

https://ml-tutorials.com/?fbclid=IwAR3Pueo8-42V7kRvisaOPWbEQ_Jqh2GPiN6xbr_NMUQXd3o0jE3ScGq5Ows

https://arxiv.org/abs/2111.01353?fbclid=IwAR0yH9yXBaydo2AQxFe8O5W7VWSuq6drqtpXXvIeKtjImpO2bLqk44MGl1A

https://ibrahimsobh.github.io/Transformers/?fbclid=IwAR2p_W2Tb7FtdIWV4QN9ok65kf-aooQ61P2acS9uj6NS0P1XWj4xQOiomtM

https://arxiv.org/abs/2111.06091?fbclid=IwAR3ulDviDzBBU_jo2_fMqZ8oMj0se-cvroxhliHTdB9Brc_T2a1xY93j8Mc

https://medium.com/@reachtoanamikasingh19/deep-reasoning-is-this-the-next-era-of-ai-4b61b076db27

?fbclid=IwAR36Rjqbcx2rypNqipY9HsZBBUX9O4QO4A81nJrKYeJHmm4FyqRKTCrkINs

https://www.marktechpost.com/2021/11/06/microsoft-ai-introduces-turing-bletchley-a-2-5-billion-parameter-universal-image-language-representation-model-t-uilr/?fbclid=IwAR3c-ZVKom5OxRyiCE4Rype6dCze59uElnk4hq7jplci0FV_d0TT1bbwupQ



https://thegradient.pub/explain-yourself/?fbclid=IwAR0p-fyZMtncqx3Tv-8LXW0gtwo8HfI1kr5zOGuHgy5RtRwPY4Gzyoi6MqA

https://github.com/soulmachine/machine-learning-cheat-sheet?fbclid=IwAR25CONiORrXXiQKFvfZhO-cSFuKP-WzYsVdusDDEJYB0c7ZZE2CjSerPnI

-->













<!--ssssssssssssssssssssssssssssssssssssssssss-->
<!--S*S--><div class="slide">
<!--t*s--><div id="itframe"><h1 class="st">
Our 'menu' for today
<!--t*e--></h1></div>
<!--c*s-->

<p>We humans (not other animals!) have been to outer space, invented agriculture, found cures for diseases, invented countless things, create and use STEM, have radically altered the environment... But, these pale in comparison, when it comes to the promise/potential/dream of machine intelligence. 

<p>Our specific topic today - a DATA-DRIVEN approach to AI. 


<p>We will necessarily leave out the underlying <a href="https://www.google.com/search?rlz=1C1CHBF_enUS723US723&biw=1280&bih=578&tbm=isch&sxsrf=ACYBGNSypnGGsQCmlR06KdIH6k5Yk9WqwQ%3A1573682384208&sa=1&ei=0HzMXcmjDN_F0PEP6pOngAc&q=machine+learning++math&oq=machine+learning++math&gs_l=img.3..0l5j0i5i30j0i8i30l4.8773.13168..14131...0.0..0.106.1518.16j1......0....1..gws-wiz-img.......0i67j0i10j0i7i30j0i7i5i30j0i8i7i30.YDGHzCxzHz8&ved=0ahUKEwjJy__bl-jlAhXfIjQIHerJCXAQ4dUDCAc&uact=5">math</a> - courses related to ML (CS566, CS567...) will provide you that [almost all the math falls into these three categories: <b>statistics, probability</b> - for data sampling, experiment design, simulation, model building; <b>linear algebra</b> - for data description and analysis (eg. in NNs), geometric operations on data; <b>calculus</b> - for function optimization (eg error reduction, reward maximization)]


<p><a href="https://www.raconteur.net/infographics/a-brief-history-of-ai/">Here</a> is 'a' history of AI, to help set the stage. What's not shown, is the 'Cyc' project ('84-'94), which I worked on (for just a year). 


<!--c*e-->
<!--S*E--></div>
<!--eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee-->




<!--ssssssssssssssssssssssssssssssssssssssssss-->
<!--S*S--><div class="slide">
<!--t*s--><div id="itframe"><h1 class="st">
A couple of clips...
<!--t*e--></h1></div>
<!--c*s-->
<p>Knosis.ai: https://www.youtube.com/watch?v=3RJ_YPh-1t8 - glimpses of how ML is FUELED by data!!

<p>A fascinating documentary on (data-driven) ML: https://www.pbs.org/wgbh/frontline/film/in-the-age-of-ai - ~2 hours, every second of which is worth watching (because this is how our future is being shaped). Set aside 2 hours, watch it; or, as my friend Lurong would exclaim, "just-tu do it!!" :) <b>For now, we simply want a TL;DR - so, let's watch just till 1:15.</b>

<!--c*e-->
<!--S*E--></div>
<!--eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee-->








<!--ssssssssssssssssssssssssssssssssssssssssss-->
<!--S*S--><div class="slide">
<!--t*s--><div id="itframe"><h1 class="st">
Types of AI, Machine Learning  ("ML"), types of ML
<!--t*e--></h1></div>
<!--c*s-->
<p>Here is a good way [after Arend Hintze] to <b>classify AI types</b> (not just techniques!)..

<p><b>Type I</b>: Reactive machines - make optimal moves - no memory, no past 'experience'. Ex: game trees. 

<p><b>Type II</b>: Limited memory - human-compiled/provided , one-shot 'past' 'experiences' are stored for lookup. Ex: expert systems, neural networks.

<p><b>Type III</b>: Theory of Mind - "the understanding that people, creatures and objects in the world can have thoughts and emotions that affect the AI programs' own behavior". 

<p><b>Type IV</b>: Self-awareness - machines that have consciousness, that can form representations about themselves (and others).

<p>Type I AI is simply, application of rules/logic (eg. chess-playing machines).

<p>Type II AI is where we are, today - specifically, this is what we call 'machine learning' - it is <b>"data-driven AI"</b>! Within the last decade or so, spectacular progress has been made in this area, ending what was called the 'AI Winter'. 

<p>As of now, types III and IV  are in the realm of speculation and science-fiction, but in the general public's mind, they appear to be certainty in the near term :)


<p>Practically speaking, there are exactly three types of AI that have been pursued, in the quest for human-level AI:
<ul>
<li>inference-based: 'symbolic'
<li>goals/rewards-based: 'reinforcement'
<li>connection-based: 'neuro'
</ul>


<hr>



<p>ML is the ONE subset of AI that is revolutionizing the world.

<p>"Machine learning focuses on the construction and study of systems that can learn from data to optimize a performance function, such as optimizing the expected reward or minimizing loss functions. The goal is to develop deep insights from data assets faster, extract knowledge from data with greater precision, improve the bottom line and reduce risk."
<br>  - Wayne Thompson, SAS

<p>ML comes in several flavors - the key types of machine learning include: 
<ul>
<li> Supervised learning
<li> Unsupervised learning
<li> Semisupervised learning
<li> Reinforcement learning 
</ul>

<p>Here is a classification:
<br><img src="pics/MLTypesFlowchart.jpg">

<p><b>Supervised learning</b> algorithms are "trained" using  examples (<b>DATA!</b>] where in addition to features [inputs], the desired output [label, aka target] is known. The goal is to LEARN the patterns inherent in the training dataset, and use the knowledge to PREDICT the labels for new data. 

<p><b>Unsupervised learning</b> is a type of machine learning where the system operates on unlabeled examples. In this case, the system is not told the "right answer." The algorithm tries to find a hidden structure or manifold in unlabeled data. The goal of unsupervised learning is to explore the data to find intrinsic structures within it using methods like clustering or dimension reduction. 

<p>For Euclidian space data: k-means clustering, Gaussian mixtures and principal component analysis (PCA)

<p>For non-Euclidian space data: ISOMAP, local linear embedding (LLE), Laplacian eigenmaps, kernel PCA.

<p>Use matrix factorization, topic models/graphs for social media data.

<p>Here is a WIRED mag writeup on unsupervised learning:
<span>
<p style="font-size:14px;">Let's say, for example, that you're a researcher who wants to learn more about human personality types. You're awarded an extremely generous grant that allows you to give 200,000 people a 500-question personality test, with answers that vary on a scale from one to 10. Eventually you find yourself with 200,000 data points in 500 virtual "dimensions" - one dimension for each of the original questions on the personality quiz. These points, taken together, form a lower-dimensional "surface" in the 500-dimensional space in the same way that a simple plot of elevation across a mountain range creates a two-dimensional surface in three-dimensional space.

<p style="font-size:14px;">What you would like to do, as a researcher, is identify this lower-dimensional surface, thereby reducing the personality portraits of the 200,000 subjects to their essential properties - a task that is similar to finding that two variables suffice to identify any point in the mountain-range surface. Perhaps the personality-test surface can also be described with a simple function, a connection between a number of variables that is significantly smaller than 500. This function is likely to reflect a hidden structure in the data.

<p style="font-size:14px;">In the last 15 years or so, researchers have created a number of tools to probe the geometry of these hidden structures. For example, you might build a model of the surface by first zooming in at many different points. At each point, you would place a drop of virtual ink on the surface and watch how it spread out. Depending on how the surface is curved at each point, the ink would diffuse in some directions but not in others. If you were to connect all the drops of ink, you would get a pretty good picture of what the surface looks like as a whole. And with this information in hand, you would no longer have just a collection of data points. Now you would start to see the connections on the surface, the interesting loops, folds and kinks. This would give you a map.
</span>

<p><a target="_blank" href="https://www.technologyreview.com/s/612427/the-rare-form-of-machine-learning-that-can-spot-hackers-who-have-already-broken-in/">Here</a> is a practical use for unsupervised learning.

<p><b>Semisupervised learning</b> is used for the same applications as supervised learning. But this technique uses both labeled and unlabeled data for training - typically, a small amount of labeled data with a large amount of unlabeled data. The primary goal is unsupervised learning (clustering, for example), and labels are viewed as side information (cluster indicators in the case of clustering) to help the algorithm find the right intrinsic data structure. 

<p>With <b>reinforcement learning</b> 'RL'), the algorithm discovers for itself which actions <a target="_blank" href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf">yield the greatest rewards</a> through trial and error. Reinforcement learning has three primary components: 
<br>1. agent - the learner or decision maker
<br>2. environment - everything the agent interacts with
<br>3. actions - what the agent can do

<p>The objective is for the agent to choose actions that maximize the expected reward over a given period of time. The agent will reach the goal much quicker by following a good policy, so the goal in reinforcement learning is to learn the best policy. Reinforcement learning is often used for robotics and navigation. 

<p>Markov decision processes (MDPs) are popular models used in reinforcement learning. MDPs assume the state of the environment is perfectly observed by the agent. When this is not the case, we can use a more general model called partially observable MDPs (or POMDPs).

<p>And there's also, hierarchical RL: https://sites.google.com/view/hrl-ep3



<!--c*e-->
<!--S*E--></div>
<!--eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee-->










<!--ssssssssssssssssssssssssssssssssssssssssss-->
<!--S*S--><div class="slide">
<!--t*s--><div id="itframe"><h1 class="st">
'Neuro' [origins of modern ML]
<!--t*e--></h1></div>
<!--c*s-->
<p>Our brains contain about <a target="_blank" href="http://www.human-memory.net/brain_neurons.html">100 billion of them</a> - each neuron is like a function, with inputs ("dendrites"), and an output ("axon"):
<p><img src="pics/neuron.jpg">

<p>Neurons ENCODE memory, learning... There are many types of neurons:
<p><img src="pics/neuron_types.jpg">

<p>A neurons CONNECTS, via dendrites (inputs) and axon (output), to other neurons:
<p><img src="pics/neurons.jpg">

<p>A neural network is a form of 'AI' - uses neuron-like connected units to <b>learn patterns</b> in training (existing) data that has known outcomes, and uses the learning to be able to gracefully respond to new (non-training, 'live') data. 

<p>Definition: a neural net(work) is <b>an interconnected set of weighted, nonlinear functions</b> [this compact definition will become clear[(er), soon]:
<br><img src="pics/NNDef.png">

<p>The overall idea is this:
<ul> 
<li>existing data is used to TRAIN a neural network - the network 'learns' patterns in the data, by adapting weights in each interconnected unit ('neuron')
<li>the network can now go 'live', ie. be deployed
<li>new data can be processed on the DEPLOYED network, which would make predictions about it based on the patterns learned 
</ul>


<p><img src="pics/neur.png">


<p>Guess why you are able to recognize these? 
<p><img src="pics/A1.jpg">
<p><img src="pics/A2.png">
<p><img src="pics/A3.jpg">

<p><img src="pics/P1.jpg">
<p><img src="pics/P2.png">
<p><img src="pics/P3.jpg">


<p>Neural networks (NNs) can be used to:

<ul>
<li>recognize/classify features - traffic, terrorists, expressions, plants, words..
<li>detect anomalies - unusual CC activity, unusual machine states, gene sequences, brain waves..
<li>predict exchange rates, 'likes'..
<li>calculate numerical values (eg. home prices)
<li>... [HUNDREDS, if not THOUSANDS, of uses - ANY form of data, that has ANY pattern in it, can be learned!!]
</ul>

<p><a target="_blank" href="https://www.youtube.com/watch?v=cNxadbrN_aI">Here</a> is some early NN work. 

<p>As you can imagine, 'Big Data' can help in all of the above! The bigger the training set, the better the learning, and therefore, better the result. 


<p>Below is an overview of how NNs work..

<p>The brain (specifically, learning/training) is modeled after strengthening relevant neuron connections - neurons communicate (through axons and dendrites) dataflow-style (neurons send output signals to other neurons):

<p><img src="pics/Brain.png">

<p>Linear (identity), 'leaky' output: input values get passed through 'verbatim' (not very useful to us, does not happen in real brains!):

<p><img src="pics/LinNeurons.png">


<p>A better model is when a neuron outputs a 1 (stays 0 to start with) ("fires") if and when its combined inputs exceed a threshold value:

<p><img src="pics/BinThreshNeurons.png">

<p>Another option is to convert the 'step' pulse to a ramp:

<p><img src="pics/RectifiedLinNeurons.png">

<p>Even better - use a smoother buildup of output:
<p><img src="pics/SigmoidNeurons.png">

<p>*Even* better - use a sigmoidal probability distribution for the output:

<p><img src="pics/StochasticBinNeurons.png">


<p>The functions we use to generate the output, are called activation functions - the ones we looked at are identity, binary threshold, rectifier and sigmoid. The gradients of these functions are used during backprop. There are more (look these up later) - symmetrical sigmoid, ie. hyperbolic tangent (tanh), soft rectifier, polynomial kernels...

<p>This is from an early ('87) newsletter - today's NNs are not viewed as systems of coupled ODEs - instead we use 'training' to make processing element 'learn' how to respond to its inputs:
<p><img src="pics/NNOld.png">

<p>With the above info, we can start to build our neural networks!

<p>* we create LAYER upon LAYER of neurons - each layer is a set (eg. column) of neurons, which feed their (stochastic) outputs downstream, to neurons in the next (eg. column to the right) layer, and so on
<p>* each layer is responsible for 'learning' some aspect of our target - usually the layers operate in a hierarchical (eg. raw pixels to curves to regions to shapes to FEATURES) fashion
<p>* a layer 'learns' like so: its input weights are adjusted (modified iteratively) so that the weights make the neurons fire when they are given only 'good' inputs. 

<p><a target="_blank" href="https://en.wikipedia.org/wiki/File:Colored_neural_network.svg">Here</a> is how to visualize the layers.

<p>The above steps can be summarized this way:

<p><img src="pics/NNLearning.png">

<p><b>Learning (ie. iterative weights modification/adjustment) works via 'backpropagation', with iterative weight adjustments starting from the last hidden layer (closest to the output layer) to the first hidden layer (closest to the input layer). Backpropagation aims to reduce the ERROR between the expected and the actual output [by finding the minimum of the [quadratic] loss function], for a given training input. Two hyper/meta parameters guide convergence: learning rate [scale factor for the error], momentum [scale factor for error from the previous step]. To know more (mathematical details), look at <a target="_blank" href="https://towardsdatascience.com/estimating-optimal-learning-rate-for-a-deep-neural-network-ce32f2556ce0">this page</a>, and <a target="_blank" href="http://cs231n.github.io/neural-networks-3/">this</a>.
</b>

<p>Here is backprop again, in equation and code form:
<br><img src="pics/BLpseudo.png">
<br><img src="pics/BLcode.png">


<p>To quote MIT's Alex "Sandy" Pentland: "The good magic is that it has something called the credit assignment function. What that lets you do is take stupid neurons, these little linear functions, and figure out, in a big network, which ones are doing the work and encourage them more. It's a way of taking a random bunch of things that are all hooked together in a network and making them smart by giving them feedback about what works and what doesn't. It sounds pretty simple, but it's got some complicated math around it. That's the magic that makes AI work."

<p>As per the above, here is a schematic showing how we could look for a face:
<p><img src="pics/AFaceOrNot.png">


<p>Note that a single neuron's learning/training (backprop-based calculation of weights and bias) can be considered to be equivalent to multi-linear regression - the neuron's inputs are features (x_0, x_1..), the learned weights are corresponding coefficients (w_0,w_1..) and the bias 'b' is the y intercept! We then take this result ('y') and non-linearize it for output, via an activation function. So overall, this is equivalent to applying logistic regression to the inputs. When we have multiple neurons in multiple layers (all hidden, except for inputs and outputs), we are chaining multiple sigmoids, which can approximate ANY continuous function! THIS is the true magic of ANNs. Such 'approximation by summation' occurs elsewhere as well - the Stone-Weierstrass theorem, Fourier/wavelet analysis, power series for trig functions...

<p>A simpler example - a red or blue classifier can trained, by feeding it a large set of (x,y) values and corresponding blueness values - the learned weights in this case are the coefficients a and b, in the line equation ax+by=c [equivalently, m and c, in y=mx+c]:
<p><img src="pics/RedOrBlue.png">

<p>Here is a simple network to learn XOR(A,B) - here all the 6 weights (1,1,1,1,-1,1) are learned:
<p><img src="pics/XOR.png">
<p>The following clip shows how a different NN (with one middle ('hidden') layer with 5 neurons) learns XOR - as the 5 neurons' weights (not pictured) are repeatedly modified, the 4 inputs ((0,0), (0,1), (1,0), (1,1)) progressively lead to the corresponding expected XOR values of 0,1,1,0 [in other words, the NN learns to predict XOR-like outputs when given binary inputs, just by being provided the inputs as well as expected outputs]:
<p><video controls=true src="clips/XOR.mp4">
<p><a target="_blank" href="https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/">This</a> page has clear, detailed steps on weights updating. 

<p>In the above examples, there was a single neuron at the output layer, with a single 0 to 1 probability value as its output; if we had multiple neurons (one for each class we want to identify), we'd like their probabilities to sum up to 1.0 - we'd then use a <a target="_blank" href="https://en.wikipedia.org/wiki/Softmax_function">'Softmax' classifier</a> [a generalization of the sigmoid classifier shown above]. A Softmax classifier takes an array of 'k' real-valued inputs, and returns an array of 'k' 0..1 outputs that sum up to 1.




<p>NN-based learning has started to REVOLUTIONIZE AI, thanks to three advances:
<ul>
<li> Big Data (BILLIONS of images, tens of thousands of hours of video/audio, terabytes of text, billions of tweets..), to use for training: more training predictably leads to better learning 
<li>better algorithms - fruits of decades' worth of academic research in ML; more recently, 'industry' (Google, Facebook, Microsoft, IBM) seems to be taking the lead
<li>faster cloud computing platforms and libraries 
</ul>

<p><a target="_blank" href="https://github.com/dennybritz/nn-from-scratch/blob/master/nn-from-scratch.ipynb
">Here</a> is a (Jupyter) notebook with an NN implementation, if you want to play with it [you can simply look at the rendered/static version on GitHub, or <a target="_blank" href="https://www.dataschool.io/cloud-services-for-jupyter-notebook/">interact</a> with it]. 

<p>'Summary':
<br><img src="pics/BioAI.png">

<br><img style="" src="pics/LinNonlin.png">

<p>And, REMEMBER:
<br><img src="pics/linchpin.png">




<p><img style="width:50%" src="pics/MLPipeline.jpg">

<p>What we do in (supervised) ML is IDENTICAL to what we do in BI, DM!

<p>It's ALL about <b>calculating quantities derived from patterns in existing data</b>. 



<p>EVERY neural network (which is really (supervised) ML is) is simply, a giant, deterministic, non-linear EQUATION!!!

<p><img style="width:50%" src="pics/fn.png">
<p><img style="width:50%" src="pics/fn2.png">

<p>(x0,x1,x2...) is a single piece (row) of data. Given it, WHAT IS 'y'? In other words, <b>WHAT IS f()? [wtf, lol]</b>

<p>How can we calculate f()?
<ul>
<li>using LOTS of data 
<li>by iteration - via 'hyper params' such as architecture, learning rate, momentum; by optimizing our solving; by computing and using error between computed and expected outputs
<li>using nonlinearity 
<li>using LOTS of small, simple 'neuron' subfunctions (out of which our f() is COMPOSED) [connected using specific *architectures*]
</ul>








<p><b>'Deep Learning'</b> is starting to yield spectacular results, to what were once considered intractable problems..

<p>Why now? Massive amounts of learnable data, massive storage, massive computing power, advances in ML.. <a target="_blank" href="https://www.youtube.com/watch?v=roCXXvI5wK4">Here</a> is NVIDIA's response (to 'why now')..

<p>In Deep Learning, we have large numbers (even 1000!) of hidden layers, each of which learns/processes a single feature.
Eg. here is a (non-so-deep) NN:
<p><img src="pics/DeepLearning_1.png">
<p>"Deep learning is currently one of the best providers of solutions regarding problems in image recognition, speech recognition, object recognition, and natural language with its increasing number of libraries that are available in Python. The aim of deep learning is to develop deep neural networks by increasing and improving the number of training layers for each network, so that a machine learns more about the data until it's as accurate as possible. Developers can avail the techniques provided by deep learning to accomplish complex machine learning tasks, and train AI networks to develop deep levels of perceptual recognition."

<p>Q: so what makes it 'deep'? A: the number of intermediate layers of neurons.

<p>Deep learning is a <a target="_blank" href="http://blog.algorithmia.com/ai-why-deep-learning-matters/">"game changer"..</a>
<!--c*e-->
<!--S*E--></div>
<!--eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee-->


<!--ssssssssssssssssssssssssssssssssssssssssss-->
<!--S*S--><div class="slide">
<!--t*s--><div id="itframe"><h1 class="st">
RNN, LSTM, Transformers
<!--t*e--></h1></div>
<!--c*s-->

<p>An RNN is a history-dependent network where past predictions are used for future ones (by having outputs fed back):

<br><img src="pics/RNN.png">

<p>An LSTM is a special kind of RNN, for being able to process longer chains of dependencies.
<br><img src="pics/LSTM.png">

<p>RNNs/LSTMs are especially good for 'sequence' problems such as speech recognition, language translation, etc.; they are not massively parallelizable the way CNNs can be.

<p><a href="https://naokishibuya.medium.com/long-short-term-memory-394aa8461a35
">Here</a> is more on LSTMs. 

<p>Temporal Convolution Nets (TCNs) are a good, parallelizable alt to RNNs; Numenta's <a target="_blank" href="https://numenta.com/press/2017/06/20/numenta-publishes-in-neurocomputing/">HTM</a> - also a better alternative to RNNs.



<!--c*e-->
<!--S*E--></div>
<!--eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee-->




<!--ssssssssssssssssssssssssssssssssssssssssss-->
<!--S*S--><div class="slide">
<!--t*s--><div id="itframe"><h1 class="st">
NN architectures
<!--t*e--></h1></div>
<!--c*s-->
<p>Specific architectures (numbers and types of layers) exist, for different NN tasks - eg. look at this page: https://medium.com/@sidereal/cnns-architectures-lenet-alexnet-vgg-googlenet-resnet-and-more-666091488df5

<p>Before embarking on a big task, it is important to first identify, or create, a suitable architecture - otherwise, learning efficiency, and/or performance accuracy, will suffer.
<!--c*e-->
<!--S*E--></div>
<!--eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee-->


<!--ssssssssssssssssssssssssssssssssssssssssss-->
<!--S*S--><div class="slide">
<!--t*s--><div id="itframe"><h1 class="st">
CNN [Convolutional Neural Network]
<!--t*e--></h1></div>
<!--c*s-->
<p>
<p>In signal processing, a convolution is a blending (or integrating) operation between two functions (or signals or numerical arrays) - one function is convolved (pointwise-multiplied) with another, and the results summed. 

<p>Here is an example of convolution - the 'Input' function [with discrete array-like values] is convolved with a 'Kernel' function [also with a discrete set of values] to produce a result; here this is done six times:

<p><img src="pics/conv.png">


<p>Convolution is used heavily in creating image-processing filters for blurring, sharpening, edge-detection, etc. The to-be-processed image represents the convolved function, and a 'sliding' "mask" (grid of weights), the convolving function (aka convolution kernel):

<p><img src="pics/conv1.png">

<p>Here is [the result of] a blurring operation:
<p><img src="pics/conv2.png">

<p><a target="_blank" href="https://www.html5rocks.com/en/tutorials/canvas/imagefilters/">Here</a> you can fill in your own weights for a kernel, and examine the resulting convolution.

<p>So - how does this relate to neural nets? In other words, what are CNNs?

<p>CNNs are biologically inspired - (convo) filters are used across a whole layer, to enable the entire  layer as a whole to detect a feature. Detection regions are overlapped, like with cells in the eye.

<p><a target="_blank" href="pics/LeCun.pdf">Here</a> is an*excellent* talk on CNNs/DNNs, by Facebook's LeCun.

<p><a target="_blank" href="http://colah.github.io/">Here</a> is a *great* page, with plenty of posts on NNs - with lots of explanatory diagrams.

<p>In essence, a CNN is where we represent a neuron's weights as a matrix (kernel), and slide it (IP-style) over an input (an image, a piece of speech, text, etc.) to produce a convolved output.

<p>In what sense is a neuron's weights, a convolution kernel? 

<p>We know that for an individual neuron, its output `y` is expressed by

<p>
`y = x_0*w_0 + w_1*x_1 + .... + w_n*x_n + b`, where the `w_i`s represent the neuron's weights, and the `x_i`s, the incoming signals [`b` is the neuron's activation bias]. The multiplications and summations resemble a convolution! The incoming 'function' is `[x_0, x_1, x_2, .... x_n]`, and the neuron's kernel 'function', `[w_0, w_1, w_2, .... w_n]`.

<p>Eg. if the kernel function is `[0,0,0...w_0,w_1,0,0...]` [where we only process our two nearest inputs], the equivalent network would look like so [fig from Chris Olah]:
<p><img src="pics/colah1.png">
<p>The above could be considered one 'layer' of neurons, in a multi-layered network. The convolution (each neuron's application of `w_0` and `w_1` to its inputs) would produce the following:
<br> `y_0 = x_0*w_0 + x_1*w_1 + b_0`
<br> `y_1 = x_1*w_0 + x_2*w_1 + b_1`
<br> `y_2 = x_2*w_0 + x_3*w_1 + b_2`
<br>....

<p>Pretty cool, right? Treating the neuron as a kernel function provides a convenient way to represent its weights as an array. For 2D inputs such as images, speech and text, the kernels would be 2D arrays that are coded to detect specific features (such as a vertical edge, color..).

<p>EACH NEURON IS CONVOLVED OVER THE ENTIRE INPUT (again, IP-style), AND AN OUTPUT IS GENERATED FROM ALL THE CONVOLUTIONS. The output gets 'normalized' (eg. clamped), and 'collapsed' (reduced in size, aka 'pooling'), and the process repeats down several layers of neurons: input -> convolve -> normalize -> reduce/pool -> convolve -> normalize -> reduce/pool -> ... -> output.

<p>The following pics are from a talk by Brandon Rohrer (Microsoft). You DON'T need to know the details of the steps - just understand that PIXELs are input, classification is the output.

<p>What we want:
<p><img src="pics/BR1.png">

<p>The input can be RST (rotation, scale, translation) of the original:
<p><img src="pics/BR2.png">

<p>How can we compute similarity, but not LITERALLY (ie without pixel by pixel comparison)?
<p><img src="pics/BR3.png">

<p>Useful pixels are 1, background pixels are -1:
<p><img src="pics/BR4.png">

<p><img src="pics/BR5.png">

<p>We match SUBREGIONS:
<p><img src="pics/BR6.png">

<p>Convolutional neurons that check for these three features:
<p><img src="pics/BR7.png">

<p><img src="pics/BR8.png">

<p><img src="pics/BR9.png">

<p><img src="pics/BR10.png">

<p><img src="pics/BR11.png">

<p><img src="pics/BR12.png">

<p>CONVOLVE, ie. do `x_i*w_i`, then average, output a value:
<p><img src="pics/BR13.png">

<p><img src="pics/BR14.png">

<p><img src="pics/BR15.png">

<p><img src="pics/BR16.png">

<p><img src="pics/BR17.png">

<p><img src="pics/BR18.png">

<p><img src="pics/BR19.png">

<p>Need to center the kernel at EVERY pixel (except at the edges) and compute a value for that pixel!
<p><img src="pics/BR20.png">

<p><img src="pics/BR21.png">

<p><img src="pics/BR22.png">

<p>We end up with a 7x7 output grid, just for this (negative slope diagonal) feature:
<p><img src="pics/BR23.png">

<p><img src="pics/BR24.png">

<p>Each neuron (feature detector) produces an output - so a single input image produces a STACK of output images [three in our case, one from each feature detector]:
<p><img src="pics/BR25.png">


<p><img src="pics/BR26.png">

<p>To collapse the outputs, we do 'max pooling' - replace an mxn (eg. 2x2) neighborhood of pixels with a single value, the max of all the m*n pixels.

<p><img src="pics/BR27.png">

<p><img src="pics/BR28.png">

<p><img src="pics/BR29.png">

<p><img src="pics/BR30.png">

<p><img src="pics/BR31.png">

<p>Next, create a ReLU - rectified linear unit - replace negative values with 0s:
<p><img src="pics/BR32.png">

<p><img src="pics/BR33.png">

<p><img src="pics/BR34.png">

<p><img src="pics/BR35.png">

<p>After a single stage of convolution, ReLU, pooling (or eqvt'ly, convolution, pooling, ReLU):
<p><img src="pics/BR36.png">

<p>Usually there are multiple stages:
<p><img src="pics/BR37.png">

<p>The resulting output values (12 in our case) are equivalent to VOTES: values at #0, #3, #4, #9, #10 contribute to voting for an 'X'; by repeated training with X-like images, which produce high-valued outputs for exactly those values at #0,#3,#4,#9,#10, the RECEIVER of all the 12 values, ie . the 'X' detector, learns to adjust its weights so that those inputs at #0,#3,#4,#9,#10 matter more (get assigned higher weight multipliers) compared to the other inputs such as #1,#2..:
<p><img src="pics/BR38.png">

<p><img src="pics/BR39.png">

<p>Likewise, if we fed O image detectors kernels' results (also an array of 12 values) to the O receiver, the O receiver would classify it as an O - because the O detector has been separately trained, using several O-like images and O-feature detector neurons!!
<p><img src="pics/BR40.png">

<p>After training, a new ('test') image is fed to BOTH the X feature detector neurons AND to the O feature detector neurons, who outputs are all combined to produce a 12-element array as before. Now we feed that array to both the X-decider neuron and the O-decider neuron:

<p><img src="pics/BR41.png">

<p>Here's the output for X  and O - the results average to 0.91 for X, and 0.52 for O - the NN would therefore classify this as an X:
<p><img src="pics/BR42.png">

<p><img src="pics/BR43.png">

<p>If we feed the network an O-like image instead, the X and O detectors will go to work, and produce an output array where the O features (at #1,#2..) would be higher. So when this array is fed to the X decider and the O decider, we expect the image to be classified as O, eg. because the output probabilities from the X decider and O decider come out to be 0.42 and 0.89. 

<p>Repeat for each class that needs to be learned: test input => class detectors => outputs => train classifier.

<p>This is very roughly equivalent to creating a "regression line" that "best fits" available data.

<p>In summary: 
<p><img src="pics/BR44.png">

<p>In real world situations, these voting outputs can also be cascaded:
<p><img src="pics/BR45.png">

<p>'All together now':
<p><img src="pics/BR46.png">

<p>In the above, if we had fed an O-like image instead, the output probability would be higher for O. 

<p>Errors are reduced via backpropagation. Error is computed by taking the absolute differences' sums between expected and observed outputs:
<p><img src="pics/BR47.png">


<p>In RL, we'd use thousands of images for each class (outcome/label), and create a network that can detect dozens of classes - eg. here is a pictorial representation of an NN that can classify dogs:
<p><img src="pics/DogDet.png">

<p>For each feature, each weight (one at a time) is adjustly slightly (+ or -, using the given learning rate) from its current value, with the goal of reducing the error (use the modified weights to re-classify, recompute error, modify weights, reclassify.. iterate till convergence) - this is called backpropagation:
<p><img src="pics/BR48.png">


<p>A <a target="_blank" href="https://hackernoon.com/what-is-a-capsnet-or-capsule-network-2bfbe48769cc">Capsule Network</a> (CapsNet) is a more robust (compared to regular CNNs) architecture for object detection; see also <a target="_blank" href="https://ireneli.eu/2018/01/23/deep-learning-16-understanding-capsule-nets/">this</a> page.



<p>That was a whirlwind tour of the world of CNNs! Now you can start to understand how an NN can detect faces, cars..:
<p><img src="pics/FacesCars.png">

<p>When is a CNN **not** a good choice? Answer: when data is not spatially laid out, ie. scrambling rows and columns of the data would still keep the data intact (like in a relational table) but would totally throw off the convolutional neurons!
<!--c*e-->
<!--S*E--></div>
<!--eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee-->










<!--ssssssssssssssssssssssssssssssssssssssssss-->
<!--S*S--><div class="slide">
<!--t*s--><div id="itframe"><h1 class="st">
Players (Top Six, plus others)
<!--t*e--></h1></div>
<!--c*s-->
<p>
<ul>

<li>Amazon uses deep learning for product recommendations, Alexa...

<li>Google: self-driving cars, <a target="_blank" href="https://www.tensorflow.org/">TensorFlow</a>, <a target="_blank" href="https://deepmind.com/">DeepMind</a>


<li>Microsoft: <a target="_blank" href="http://www.wired.com/2016/01/microsoft-neural-net-shows-deep-learning-can-get-way-deeper/">ImageNet entry</a>, <a target="_blank" href="https://github.com/Microsoft/CNTK">CNTK</a>, Skype Translator... <a href="https://aidemos.microsoft.com/">Here</a> are a bunch of their AI demos.

<li>Facebook: <a target="_blank" href="pics/DeepFace.png">DeepFace</a> can search 800M faces in <5 sec! Also, Facebook is planning to <a target="_blank" href="https://code.facebook.com/posts/1687861518126048/facebook-to-open-source-ai-hardware-design/">open source</a> its hardware setup. Deep learning is also used in Instagram, for recognizing content in images, including text.

<li>IBM: <a target="_blank" href="https://www.ibm.com/smarterplanet/us/en/ibmwatson/developercloud/">Watson</a>, <a target="_blank" href="https://www-03.ibm.com/press/us/en/pressrelease/46205.wss">AlchemyAPI</a>, <a target="_blank" href="http://www.ibm.com/analytics/watson-analytics/us-en/">Watson Analytics</a>

<li>Apple uses deep learning for Siri, iTunes, etc. 

<li>Many others: Alibaba, Baidu, Tencent, Uber, Netflix, Visa, LinkedIn...

</ul>



<p>The top three players - Amazon, Google, Microsoft - all have cloud-based APIs. Others - eg. FloydHub, Paperspace... other cloud-based ML training and hosting. 

<p>The <a target="_blank" href="https://awards.acm.org/about/2018-turing">2018 Turing Award</a> was for ML. 

<p>AI (ML, really) is tranforming world economies - everyone wants to participate, and WIN: 

<ul>
<li>US: <a target="_blank" href="https://web.archive.org/web/20190430021514/https://www.whitehouse.gov/presidential-actions/executive-order-maintaining-american-leadership-artificial-intelligence/">this</a> and <a target="_blank" href="https://newsroom.intel.com/articles/intels-recommendations-u-s-national-strategy-artificial-intelligence/">this</a>

<li>China: <a target="_blank" href="https://multimedia.scmp.com/news/china/article/2166148/china-2025-artificial-intelligence/index.html">world domination</a>

<li>India: <a target="_blank" href="docs/NationalStrategy_for_AI_Discussion_Paper.pdf">address societal needs</a> 

<li>EU: <a target="_blank" href="https://www.globalgovernmentforum.com/eu-launches-ai-strategy/">strategy</a>
</ul>

<p>Again - > watch the ~ 2 hour PBS that we brought up earlier.

<p>Look up papers/blogs by:

<ul>
<li>Andrew Ng
<li>Yann LeCun
<li>Andrej Karpathy
<li>Chris Olah
<li>Brandon Rohrer
</ul>


<!--c*e-->
<!--S*E--></div>
<!--eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee-->

<!--ssssssssssssssssssssssssssssssssssssssssss-->
<!--S*S--><div class="slide">
<!--t*s--><div id="itframe"><h1 class="st">
Applications - a sampler
<!--t*e--></h1></div>
<!--c*s-->

<p>ML is a runaway <b>engineering</b> success, which is sure to lead to 1000s (!) of applications, covering every human activity! Remember - if ANYTHING has a 'PATTERN' (that a. sets it APART from others, and b. has VARIATIONS within itself), it can be LEARNED!

<p>Below is an arbitrary ("random") sampling of applications [some we talked about or encountered earlier]. The point is that "AI", ie. ML, is now mature enough, widely deployable enough that we can start dreaming up NEW USES for it!

<ul>
<li>https://www.youtube.com/channel/UCWN3xxRkmTPmbKwht9FuE5A - make the data 'lit'!
<li>SDCs, eg. https://www.youtube.com/watch?v=tiwVMrTLUWg
<li>Face detection, EU airports: https://www.cnn.com/travel/article/ai-lie-detector-eu-airports-scli-intl/index.html
<li>Face detection, Chinese classrooms(!): https://www.youtube.com/watch?v=3H1hj_C8F_A
<li>https://developer.amazon.com/blogs/alexa/post/ca34b954-1c5d-4a59-b326-f45c8df7c89c/alexa-skill-tech-for-good-challenge-winners
<li>https://ai.googleblog.com/2018/11/improved-grading-of-prostate-cancer.html
<li>ASL: https://www.youtube.com/watch?v=a4zvhJsBPa0
<li>'master key' (uh oh): https://boingboing.net/2018/11/15/masterprints.html
<li>Google's <a target="_blank" href="https://quickdraw.withgoogle.com/#">Quick Draw</a>, <a target="_blank" href="https://www.autodraw.com/">AutoDraw</a> [<a target="_blank" href="https://github.com/1991viet/QuickDraw">here</a> is an alternate implementation of QuickDraw; and, <a target="_blank" href="https://github.com/googlecreativelab/quickdraw-dataset">here</a> is Google's dataset!]
<li><a target="_blank" href="https://news.artnet.com/market/google-inceptionism-art-sells-big-439352">Inceptionism</a>, <a target="_blank" href="https://www.youtube.com/watch?v=-oLemNTo7YU">DL portrait morph</a>
<li>https://aiportraits.com/#
<li>neural style transfer [incl <a target="_blank" href="clips/Musk_StyleXfr.mp4">this</a> clip :)]
<li>deepfakes, eg. https://www.bbc.co.uk/programmes/p06r8g4l [most are NSFW!!]
<li>https://medium.freecodecamp.org/chihuahua-or-muffin-my-search-for-the-best-computer-vision-api-cbda4d6b425d, <a href="https://web.archive.org/web/20200304050004/http://www.evolvingai.org/fooling">http://www.evolvingai.org/fooling</a> [easily foolable!]
<li>https://gallery.azure.ai/browse
<li>https://lobe.ai/
<li>https://research.google.com/seedbank/seeds
<li>https://thispersondoesnotexist.com/ [and https://thisrentaldoesnotexist.com/]
<li>https://ganbreeder.app/category/random
<li>https://www.askforgametask.com/html5/tutorials/tetris_ai_bot/source/ - an agent trained to play Tetris
<li>https://teachablemachine.withgoogle.com/

<li>AlphaGo, and its spinoff company
</ul>




<!--c*e-->
<!--S*E--></div>
<!--eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee-->

<!--ssssssssssssssssssssssssssssssssssssssssss-->
<!--S*S--><div class="slide">
<!--t*s--><div id="itframe"><h1 class="st">
GANs! And encoder-decoder pairs...
<!--t*e--></h1></div>
<!--c*s-->

<p>Adversarial learning methods, [esp <a target="_blank" href="https://en.wikipedia.org/wiki/Generative_adversarial_network">GAN</a>s, that have dueling ["zero sum"] Generator  and Discriminator networks] are very interesting. 

<p><img src="pics/GAN.jpg">
<p><img src="pics/GAN2.jpg">



<p>GANs have <a target="_blank" href="https://deephunt.in/the-gan-zoo-79597dc8c347">MANY</a> variations!

<p>EBMs (a GAN alternative): https://openai.com/blog/energy-based-models/

<p>As an alternative to GANs, a similar idea, called an Encoder-Decoder pair, can ALSO generate data (faces, words, music...). The encoder, specifically a 'VAE' learns to create a representation, a 'data generating distribution', of its input data, using latent-space features [of the input data]. Roughly, it learns to map an input datum into a point in multi-dim latent space. REVERSING this, **ANY random point in the latent feature space can be used to GENERATE (via a decoder) a NEW datum!** <a href="https://www.kdnuggets.com/2021/10/introduction-autoencoder-variational-autoencoder-vae.html">Here</a> is more on VAEs.

<!--c*e-->
<!--S*E--></div>
<!--eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee-->




<!--ssssssssssssssssssssssssssssssssssssssssss-->
<!--S*S--><div class="slide">
<!--t*s--><div id="itframe"><h1 class="st">
NN via hardware, NN 'on the edge'
<!--t*e--></h1></div>
<!--c*s-->

<p>GPUs and other forms of hardware are used to accelerate deep learning - advantages: massively parallel processing, and possibility of arbitrary speed increases over time just by upgrading hardware!

<p>GPUs (multi-core, high-performance graphics chips made by NVIDIA etc.) and DNNs seem to be a match made in heaven!

<p>NVIDIA has made available a <a target="_blank" href="https://developer.nvidia.com/deep-learning-getting-started">LOT</a> of resources related to DNNs using GPUs, including a framework called  DIGITS (Deep Learning GPU Training System). NVIDIA's <a target="_blank" href="http://www.scientificcomputing.com/news/2016/04/worlds-first-deep-learning-supercomputer-launched-meet-massive-ai-demands">DGX-1</a> is a deep learning platform built atop their Tesla P100 GPUs. <a target="_blank" href="https://devblogs.nvidia.com/parallelforall/deep-learning-nutshell-core-concepts/">Here</a> is an excellent intro' to deep learning - a series of posts. <a target="_blank" href="https://www.youtube.com/watch?v=HJ58dbd5g8g">Here</a> is a GPU-powered self-driving car (with 'only' 37 million neurons) :)

<p><a target="_blank" href="http://research.microsoft.com/pubs/240715/CNN%20Whitepaper.pdf">Microsoft</a> has created a GPU-based network for doing face recognition, speech recognition, etc.

<p>Untether: https://www.technologyreview.com/the-download/613258/intel-buys-into-an-ai-chip-that-can-transfer-data-1000-times-faster/



<p>The following are GPU-based NN implementations, by others:
<ul>
<li><a target="_blank" href="http://www.anandtech.com/show/10905/amd-announces-radeon-instinct-deep-learning-2017">AMD</a>
<li><a target="_blank" href="http://www.scientificcomputing.com/news/2016/08/fujitsu-develops-high-speed-technology-process-deep-learning">Fujitsu</a>
<li><a target="_blank" href="http://www.scientificcomputing.com/news/2016/11/inspur-launches-gpu-deep-learning-appliance?cmpid=horizontalcontent">Inspur</a>
</ul>


<p>TPU (TensorFlow Processing Unit) is a Google-developed chip, for DNNs [in their Waymo cars].

<p>Intel has its <a target="_blank" href="https://www.engadget.com/2018/11/14/intel-neural-compute-stick-2/">Neural Compute Stick</a>...

<p>FPGAs also offer a <a target="_blank" href="http://www.nextplatform.com/2015/08/25/a-glimpse-into-the-future-of-deep-learning-hardware/">custom path</a> to DNN creation.

<p>Also: TeraDeep, CEVA, Synopsis, Alluviate.. 

<p>A new form of CPU, involving 'chiplets' (from AMD) might also be a suitable platform... 

<p>Intel also has <a href="https://www.intel.ai/nervana-nnp/nnpt/#gs.gf9kwv">Nervana NNP-T.</a>


<p>There is a push to also deploy models on edge devices - SoCs, smartphones, browsers...

<p>Eg. one trend is to build ML into cameras, eg. as done in <a target="_blank" href="https://www.youtube.com/watch?v=EcCbEWiyiQY">Pixy2.</a>

<p>Google's TensorFlow can also run on the <a href="https://playground.tensorflow.org">browser.</a>

<p><a href="CNNDemo/index.html">Here</a> is a <a href="https://cs231n.github.io/convolutional-networks/">ConvNet</a> (ie CNN) demo, running in the browser.

<p><a href="clips/PocketXformer.mp4">Here</a> is an example of language processing on a smartphone.

<p>Crop disease detection, in Kenya, using TF on an Android smartphone: https://www.youtube.com/watch?v=NlpS-DhayQA ...

<p>We can also do <a href="https://modeldepot.github.io/tfjs-yolo-tiny-demo/">simple object detection</a> in the browser!






<!--c*e-->
<!--S*E--></div>
<!--eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee-->


<!--ssssssssssssssssssssssssssssssssssssssssss-->
<!--S*S--><div class="slide">
<!--t*s--><div id="itframe"><h1 class="st">
XAI
<!--t*e--></h1></div>
<!--c*s-->

<p>Rather than accept that an NN is a 'blackbox', XAI attempts to crack it <a href="https://thegradient.pub/explain-yourself/ ">open.</a>

 
<!--c*e-->
<!--S*E--></div>
<!--eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee-->


<!--ssssssssssssssssssssssssssssssssssssssssss-->
<!--S*S--><div class="slide">
<!--t*s--><div id="itframe"><h1 class="st">
Architecture pruning
<!--t*e--></h1></div>
<!--c*s-->

<p>By <a href="https://towardsdatascience.com/pruning-neural-networks-1bb3ab5791f9">eliminating</a> 'weak' (small weights) connections (or entire neurons), we can retain overall accuracy, and dramatically improve performance (esp on edge devices).



 
<!--c*e-->
<!--S*E--></div>
<!--eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee-->




<!--ssssssssssssssssssssssssssssssssssssssssss-->
<!--S*S--><div class="slide">
<!--t*s--><div id="itframe"><h1 class="st">
Problems...
<!--t*e--></h1></div>
<!--c*s-->

<p>Because it's ALL based on DATA, issues arise:

<ul>
<li>bias
<li><a href="http://bit.ly/AMS2019dis">deepfakes</a>
<li>easy foolability
<li>lack of explainability
<li>unchecked power
</ul>

 
<!--c*e-->
<!--S*E--></div>
<!--eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee-->


<!--ssssssssssssssssssssssssssssssssssssssssss-->
<!--S*S--><div class="slide">
<!--t*s--><div id="itframe"><h1 class="st">
Diff eqs!
<!--t*e--></h1></div>
<!--c*s-->

<p>'Past' solutions (from t=0 to t=current) of a diff eq (ie for fluid flow, diffusion, vibration, EM propagation...) can be used as training data for an NN, which can then <a href="https://zongyi-li.github.io/blog/2020/fourier-pde/">predict</a> future evolution!

<p>Rather than have NN layers (which are discrete), why not have a continuous NN (in the mathematical sense), and <a href="https://towardsdatascience.com/neural-odes-breakdown-of-another-deep-learning-breakthrough-3e78c7213795">solve</a> for weights using ODEs? 


 
<!--c*e-->
<!--S*E--></div>
<!--eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee-->



<!--ssssssssssssssssssssssssssssssssssssssssss-->
<!--S*S--><div class="slide">
<!--t*s--><div id="itframe"><h1 class="st">
Current work (research directions)
<!--t*e--></h1></div>
<!--c*s-->
<p>
<p>Here is state-of-the-art...


<ul>

<li>the <a href="http://jalammar.github.io/illustrated-transformer/">Transformer architecture</a> <a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html">(Google, 2017)</a> is a 'game changer' for language processing - it STACKS encoders and decoders, providing longer <a href="https://arxiv.org/abs/1706.03762">'attention'</a> spans. This has given rise to HUGE pre-trained language models: <a href="https://openai.com/blog/gpt-3-apps/">GPT-3</a>, <a href="https://towardsdatascience.com/gpt-4-will-have-100-trillion-parameters-500x-the-size-of-gpt-3-582b98d82253 ">GPT-4</a>, <a href="https://venturebeat.com/2021/10/11/microsoft-and-nvidia-team-up-to-train-one-of-the-worlds-largest-language-models/">M'soft+NVIDIA</a>, <a href="https://towardsdatascience.com/gpt-3-scared-you-meet-wu-dao-2-0-a-monster-of-1-75-trillion-parameters-832cd83db484">Wu Dao 2.0</a>...





<li><a target="_blank" href="http://newsroom.ucla.edu/releases/ucla-engineers-artificial-intelligence-device-identifies-objects-speed-of-light">optics-based</a> NN

<li>another approach to AI is to model the brain's structure, in software or in hardware. IBM has its SyNAPSE chip, and <a target="_blank" href="http://www.wired.com/2014/08/ibm-unveils-a-brain-like-chip-with-4000-processor-cores/">TrueNorth</a> NN chip. Numenta is another player in <a target="_blank" href="https://discourse.numenta.org/t/breakthrough-in-construction-of-computers-for-mimicking-human-brain/4187/2">neuromorphic computing.</a> Another approach to neuromorphic chips is to <a href="https://fortune.com/2020/03/30/startup-human-neurons-computer-chips">incorporate some wetware</a> into them. 

<li>Vision Transformers! (ViT)

<li>Neuro-symbolic integration, DeepRL... [combos]

<li>Geoff Hinton: <a href="https://arxiv.org/pdf/2102.12627.pdf">GLOM</a>

<li><a href="https://www.youtube.com/watch?v=HO-_t0UArd4">GANsformers</a>

<li>synthesizing images from text descriptions, eg. DALL-E and <a href="https://towardsdatascience.com/generating-images-from-prompts-using-clip-and-stylegan-1f9ed495ddda">CLIP</a>


<li><a href="https://distill.pub/2021/gnn-intro/">GNN</a> - express data as a graph, learn the graph's structure, then predict properties given a new graph

<li><a href="https://arxiv.org/abs/2104.13478">GDL</a> - learn real-world shapes (topology)

<li>hmmm: https://spectrum.ieee.org/special-reports/the-great-ai-reckoning and https://bdtechtalks.com/2021/05/03/artificial-intelligence-fallacies/


</ul>



<!--c*e-->
<!--S*E--></div>
<!--eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee-->

<!--ssssssssssssssssssssssssssssssssssssssssss-->
<!--S*S--><div class="slide">
<!--t*s--><div id="itframe"><h1 class="st">
Summary
<!--t*e--></h1></div>
<!--c*s-->
<p>As we saw, there is a LOT going on, in ML! How to keep up?

<ul>
<li>https://arxiv.org/search/?query=ML&searchtype=all
<li>https://www.louisbouchard.ai/research-papers/
<li>https://github.com/louisfb01/best_AI_papers_2021
<li>...
</ul>

 
<!--c*e-->
<!--S*E--></div>
<!--eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee-->



<!-- ****************** -->
<!-- END END END SLIDES -->
<!-- ****************** -->
<script src="../../res/styles/slides/myslidy/bts/slidy.js"> </script>
<script src="../../res/styles/slides/myslidy/bts/setUpSlides.js"> </script>
</body>


</html>

<!-- ********************************************************************** -->
<!-- ********************************************************************** -->
<!-- ********************************************************************** -->
<!-- ********************************************************************** -->
<!-- ********************************************************************** -->







